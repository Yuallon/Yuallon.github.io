<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>机器学习之集成方法 - 愿我如星君如月，夜夜流光相皎洁！</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="雨" /><meta name="description" content="单个学习器很容易会出现过拟合 or 欠拟合问题。为了提高学习器的泛化能力，可以采取一定的策略构建并结合多个学习器来完成学习任务 &amp;ndash; 集成学习（Ense" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.69.0 with theme even" />


<link rel="canonical" href="https://yuallon.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.c7bc1becf36bcf6a9ebd25d2947e43a2eb745ddb0c9a32b43126fd7fa460c351.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="机器学习之集成方法" />
<meta property="og:description" content="单个学习器很容易会出现过拟合 or 欠拟合问题。为了提高学习器的泛化能力，可以采取一定的策略构建并结合多个学习器来完成学习任务 &ndash; 集成学习（Ense" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yuallon.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95/" />
<meta property="article:published_time" content="2020-11-24T19:10:44+08:00" />
<meta property="article:modified_time" content="2020-11-24T19:10:44+08:00" />
<meta itemprop="name" content="机器学习之集成方法">
<meta itemprop="description" content="单个学习器很容易会出现过拟合 or 欠拟合问题。为了提高学习器的泛化能力，可以采取一定的策略构建并结合多个学习器来完成学习任务 &ndash; 集成学习（Ense">
<meta itemprop="datePublished" content="2020-11-24T19:10:44&#43;08:00" />
<meta itemprop="dateModified" content="2020-11-24T19:10:44&#43;08:00" />
<meta itemprop="wordCount" content="5285">



<meta itemprop="keywords" content="集成方法," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="机器学习之集成方法"/>
<meta name="twitter:description" content="单个学习器很容易会出现过拟合 or 欠拟合问题。为了提高学习器的泛化能力，可以采取一定的策略构建并结合多个学习器来完成学习任务 &ndash; 集成学习（Ense"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">雨</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">标签</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">分类</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">雨</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">标签</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">分类</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">机器学习之集成方法</h1>

      <div class="post-meta">
        <span class="post-time"> 2020-11-24 </span>
        <div class="post-category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"> 机器学习 </a>
            </div>
          <span class="more-meta"> 5285 words </span>
          <span class="more-meta"> 11 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#个体学习器与集成学习">个体学习器与集成学习</a></li>
        <li><a href="#集成学习方法">集成学习方法</a></li>
        <li><a href="#组合策略">组合策略</a></li>
        <li><a href="#多样性">多样性</a></li>
        <li><a href="#参考资料">参考资料</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>单个学习器很容易会出现过拟合 or 欠拟合问题。为了提高学习器的泛化能力，可以采取一定的策略构建并结合多个学习器来完成学习任务 &ndash; <strong>集成学习（Ensemble Learning）</strong>，有时也被称为多分类器系统（multi-classifer system）、基于委员会的学习（committee-based learning），都是为了描述多个学习器共同作用。</p>
<h3 id="个体学习器与集成学习">个体学习器与集成学习</h3>
<h4 id="个体学习器individual-learner">个体学习器（Individual learner）</h4>
<p>通常由一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网络算法等。</p>
<h4 id="集成学习">集成学习</h4>
<p>根据不同类型个体学习器的组合方式可以分为：同质（homogeneous）集成和异质（heterogeneous）集成。</p>
<ul>
<li><strong>同质集成</strong>：只包含同种类型的个体学习器。同质集成中的个体学习器称为 <strong>基学习器（base learner）</strong>，相应的算法称为 <strong>基学习算法（base learning algorithm）</strong>。</li>
<li><strong>异质集成</strong>：包含不同类型的个体学习器。个体学习器称为 <strong>组件学习器（component learner）</strong>。</li>
</ul>
<p><img src="/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" alt="集成学习示意图"></p>
<p>考虑一个简单例子：在二分类任务中，假定3个分类器在测试样本上的表现如图8.2所示，其中✅表示分类正确，❎表示分类错误，集成学习的结果通过投票法产生，少数服从多数。</p>
<p><img src="/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E5%9B%BE8.2.jpg" alt="集成学习图8.2"></p>
<ul>
<li>（a）：每个分类器只有66.6%精度，集成学习精度达到了100%</li>
<li>（b）：3个分类器没有差别，集成之后性能没有提高。</li>
<li>（c）：每个分类器精度都只有33.3%，集成学习之后的结果变得更糟。</li>
</ul>
<p>结论：要获得好的集成，个体学习器应 <strong>好而不同</strong>，即个体学习器要有一定的 <strong>准确性</strong>（a, c）学习器不能太坏，并且要有 <strong>多样性</strong> （b中学习器没有多样性）学习器间具有差异。</p>
<p>集成学习通过将多个学习器进行结合，常常可以获得比单一学习器显著优越的泛化性能。这对 <strong>弱学习器（weak learner）</strong> 尤为明显。</p>
<ul>
<li>弱学习器：常指泛化性能略优于随机猜测的学习器。例如，二分类问题上精度略高于50%的分类器。</li>
</ul>
<h3 id="集成学习方法">集成学习方法</h3>
<p>根据个体学习器的生产方式，目前的集成学习方法大致可以分为两类：</p>
<ul>
<li><strong>Boosting</strong>：个体学习器之间存在强依赖关系，必须串行生成的序列化方法。</li>
<li><strong>Bagging and Random Forest</strong>：个体学习器之间不存在强依赖关系，可同时生成的并行方法。</li>
</ul>
<h4 id="boosting">Boosting</h4>
<p>工作机制：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直到基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。</p>
<p>Boosting是一族可将弱学习器提升为强学习器的算法。最著名的代表是 AdaBoost。</p>
<p><img src="/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0AdaBoost.jpg" alt="集成学习AdaBoost"></p>
<p>从偏差-方差分解的角度看，Boosting 主要关注降低偏差，因此 Boosting 能基于泛化性能相当弱的学习器构建出很强的集成。<strong>标准AdaBoost只适用于二分类任务</strong>。</p>
<h4 id="bagging-与-random-forest">Bagging 与 Random Forest</h4>
<p>从前面可知，如果想得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立。虽然 <strong>独立</strong> 在现实任务中无法得到，但可以设法使基学习器尽可能具有较大的差异。</p>
<p>给定一个训练数据集，为了获取有差异的基学习器，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器。由于训练数据不同，我们获得的基学习器有望具有比较大的差异。同时，为了保证个体学习器不能太差，即训练数据子集尽可能大，我们采用相互有交叠的采样子集。</p>
<p>#####Bagging</p>
<p>Bagging 是并行式集成学习方法最著名的代表。</p>
<p>给定包含m个样本的数据集，我们先随机取出1个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样经过m次随机采样操作，我们得到包含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的从未出现，理论上有63.2%的样本出现在采样集里面。这称之为 <strong>有放回重复采样（bootstrap sampling）</strong>。</p>
<p>基于 有放回重复采样，我们可以采样出T个含有m个训练样本的采样集，然后基于每个采样集训练一个出一个基学习器，再将这些基学习器进行结合，这个方法称为 <strong>Bagging集成</strong>。</p>
<ul>
<li>对分类任务使用简单投票法，如有票数相同情况，随机选择一个，也可以进一步考察学习器投票的置信度来确定。</li>
<li>对回归任务使用简单平均法。</li>
</ul>
<p><img src="/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0Bagging.jpg" alt="集成学习Bagging"></p>
<p>与标准AdaBoost只适用于二分类任务不同，Bagging能不经修改地用于多分类、回归任务。</p>
<p>有放回采样的另一个优点是：剩余的36.8%的样本可用作验证集对泛化性能进行 <strong>包外估计（out-of-estimate）</strong>。</p>
<p>从 <strong>偏差-方差分解的角度</strong> 看， Bagging 主要关注<strong>降低方差</strong>。因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更为明显。</p>
<h5 id="random-forest">Random Forest</h5>
<p>随机森林（Random Forest）是 Bagging 的一个扩展变体。<strong>RF 在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机属性选择</strong>。</p>
<p>具体来说，传统决策树在选择划分属性时是在当前节点的属性集合（假定有$d$个属性）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含$k$个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入度：若令 $k=d$，则基决策树的构建与传统决策树相同；若令 $k=1$，则是随机选择一个属性用于划分；一般情况下，推荐值 $k=log_2d$。</p>
<p>随机森林简单、容易实现、计算开销小，在很多现实任务中展现出更强大的性能，被誉为 “代表集成学习技术水平的方法”。</p>
<p>随机森林对Bagging只做了小改动，但是与Bagging中基学习器的 <strong>多样性</strong> 仅通过样本扰动（通过对初始训练集采样）而不同，<strong>随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动</strong>，这就使得最终集成的泛化性能可通过 <strong>个体学习器之间差异度的增加（核心）</strong> 而进一步提升。</p>
<h3 id="组合策略">组合策略</h3>
<p>学习器结合可能会从3各方面带来好处：</p>
<ol>
<li>单个学习器可能因误选而导致泛化性能不佳，结合多个学习器则会减少这一风险（统计角度）</li>
<li>学习算法往往会陷入局部极小，有的局部极小点所对应的泛化性能可能很糟糕，而通过多次运行之后进行结合，可降低陷入糟糕局部极小点的风险（计算角度）</li>
<li>某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器肯定无效，而通过结合多个学习器，由于相应的假设空间有所扩大，有可能得到更好的近似（假设空间角度）</li>
</ol>
<h4 id="平均法">平均法</h4>
<p>对 <strong>数值型输出</strong> ，最常见的结合策略是使用平均法（averaging）。</p>
<ul>
<li>
<p>简单平均法（simple averaging）</p>
<p>$H(x)=\frac{1}{T}\displaystyle\sum_{i=1}^{n}h_i(x)$</p>
</li>
<li>
<p>加权平均法（weighted averaging）</p>
<p>$H(x)=\displaystyle\sum_{i=1}^{n}w_ih_i(x)$</p>
<p>其中 $w_i$ 是个体学习器 $h_i$ 的权重</p>
<p>通常要求：$w_i\geq0,\displaystyle\sum_{i=1}^{n}w_i=1$</p>
</li>
</ul>
<p>集成学习中的各种结合方法都可视为其特例or变例。事实上，加权平均法可认为是集成学习研究的基本出发点，对给定的基学习器，不同的集成学习方法可视为通过不同的方式来确定加权平均法中的基学习器权重。</p>
<p>####投票法</p>
<p>对 <strong>分类</strong> 任务来说，最常见的结合策略是使用投票法。学习器 $h_i$ 将从类标记集合 ${c_1, c_2, &hellip;, c_N}$ 中预测出一个标记。将 $h_i$ 在样本上的预测输出表示为一个N维向量 $(h_i^1(x),h_i^2(x),&hellip;,h_i^N(x))$ 其中 $h_i^j(x)$ 是 $h_i$ 在类别标记 $c_j$ 上的输出。</p>
<ul>
<li>
<p><strong>绝对多数投票法（majority voting）</strong></p>
<p>$H(x)=\begin{cases}c_j,\ if\ \displaystyle\sum_{i=1}^Th_i^j(x)&gt;0.5\displaystyle\sum_{k=1}^N\sum_{i=1}^Th_i^k(x);\ reject,\ otherwise\end{cases}$</p>
<p>若某个标记的票过半数，则预测为该标记，否则拒绝预测。</p>
</li>
<li>
<p><strong>相对多数投票法（plurality voting）</strong></p>
<p>$H(x)=c_{arg\ max(j)\sum_{i=1}^Th_i^j(x)}$</p>
<p>预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个。</p>
</li>
<li>
<p><strong>加权投票法（weighted voting）</strong></p>
<p>$H(x)=c_{arg\ max(j)\sum_{i=1}^Tw_ih_i^j(x)}$</p>
<p>与加权平均法类似，$w_i$ 是 $h_j$ 的权重，通常 $w_i\geq0,\displaystyle\sum_{i=1}^Nw_i=1$。</p>
<p>将所有模型预测样本为某一类别的概率的平均值作为标准，概率最高的对应的类型为最终的预测结果，也称 <strong>加权投票法</strong>。</p>
</li>
</ul>
<p>上面的投票方式没有限制个体学习器输出值的类型。在现实任务中，不同类型个体学习器可能产生不同类型的 $h_i^j(x)（第i个学习器对第j个样本的预测结果）$，常见的有：</p>
<ul>
<li><strong>类标记</strong>：$h_i^j(x)\in{0,1}$，若成功预测为1，否则为0.使用类标记的投票亦称 <strong>硬投票（hard voting）</strong>。</li>
<li><strong>类概率</strong>：$h_i^j(x)\in[0,1]$，相当于对后验概率 $P(c_j|x)$ 的一个估计。使用类概率的投票亦称 <strong>软投票（soft voting）</strong>。</li>
</ul>
<p>例子：</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">A</th>
<th align="center">B</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">模型1</td>
<td align="center">99%</td>
<td align="center">1%</td>
</tr>
<tr>
<td align="center">模型2</td>
<td align="center">49%</td>
<td align="center">51%</td>
</tr>
<tr>
<td align="center">模型3</td>
<td align="center">40%</td>
<td align="center">60%</td>
</tr>
<tr>
<td align="center">模型4</td>
<td align="center">90%</td>
<td align="center">10%</td>
</tr>
<tr>
<td align="center">模型5</td>
<td align="center">30%</td>
<td align="center">70%</td>
</tr>
</tbody>
</table>
<p>后面百分比表示模型预测的概率。</p>
<ul>
<li>
<p>Hard voting：A - 2票，B - 3票；最终结果为B。</p>
</li>
<li>
<p>Soft voting：</p>
<p>A  - $(0.99+0.49+0.4+0.9+0.3)/5=0.616$</p>
<p>B - $(0.01+0.51+0.6+0.1+0.7)/5=0.384$</p>
<p>这里个体学习其权重都为 $1/5$ ，最终结果为A。</p>
</li>
</ul>
<p>对比：Hard voting投票方式最终结果不是由概率值更大的模型1和模型4决定，而是由概率值相对较小的模型来决定。</p>
<h4 id="学习法">学习法</h4>
<p>当训练数据很多时，一种更为强大的结合策略是使用 <strong>学习法</strong>，即通过另一个学习器来进行结合。Stacking 是学习法的典型代表。</p>
<ul>
<li>初级学习器：原始的个体学习器</li>
<li>次级学习器or元学习器（meta-learner）：用于结合的学习器。</li>
</ul>
<p>Stacking 先从初始数据集训练出初级学习器，然后 <strong>生成</strong> 一个新数据集用于训练次级学习器。初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。</p>
<p><img src="/stacking.jpg" alt="stacking"></p>
<p>下面以K- Fold交叉验证为例：</p>
<p><img src="/stacking1.jpg" alt="stacking1"></p>
<p>最后获得次级训练集为 $M（训练集样本个数）\times N（初始算法个数）$ 矩阵数据大小。</p>
<h3 id="多样性">多样性</h3>
<p>欲构建泛化性能强的集成，个体学习器应 <strong>好而不同</strong>。</p>
<p><strong>误差-分歧分解（error-ambiguity decomposition）</strong> 表示个体学习器准确性越高、多样性越大，则集成越好。</p>
<h4 id="多样性度量diversity-measure">多样性度量（diversity measure）</h4>
<p>多样性度量是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度。典型的做法是 <strong>考虑个体分类器的两两相似/不相似性</strong>。</p>
<p>给定数据集 $D={(x_1,y_1),(x_2,y_2),&hellip;,(x_m,y_m)}$，对二分类任务，$y_i\in{-1,+1}$，分类器 $h_i$ 与 $h_j$ 的预测结果列联表（contingency table）为：</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">$h_i=+1$</th>
<th align="center">$h_j=-1$</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">$h_j=+1$</td>
<td align="center">$a$</td>
<td align="center">$c$</td>
</tr>
<tr>
<td align="center">$h_j=-1$</td>
<td align="center">$b$</td>
<td align="center">$d$</td>
</tr>
</tbody>
</table>
<p>其中，$a$ 表示 $h_i$ 与 $h_j$ 均预测为正类的样本数目；$b,c,d$ 含义由此类推；$a+b+c+d=m$ 。基于这个列联表，下面给出一些常见的多样性度量。</p>
<ul>
<li>
<p>不合度量（disagreement measure）</p>
<p>$dis_{ij}=\frac{b+c}{m}$</p>
<p>$dis_{ij}$ 的值域为 $[0,1]$，值越大则多样性越大。</p>
</li>
<li>
<p>相关系数（correlation coefficient）</p>
<p>$\rho_{ij}=\frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}}$</p>
<p>$\rho_{ij}$ 的值域为 $[-1,1]$。若 $h_i$ 与 $h_j$ 无关，则值为0；若 $h_i$ 与 $h_j$ 正相关则值为正，否则为负。</p>
</li>
<li>
<p>Q-统计量（Q-statistic）</p>
<p>$Q_{ij}=\frac{ad-bc}{ad+bc}$</p>
<p>$Q_{ij}$ 与 $\rho_{ij}$ 的符号相同，且 $|Q_{ij}|\leq|\rho_{ij}|$。</p>
</li>
<li>
<p>$\kappa-$统计量（$\kappa-$statistic）</p>
<p>$\kappa=\frac{p_1-p_2}{1-p_1}$</p>
<p>其中，p1是两个分类器取得一致的概率；p2是两个分类器偶然达成一致的概率，它们可由数据集D估算：</p>
<p>$p_1=\frac{a+d}{m}$</p>
<p>$p_2=\frac{(a+b)(a+c)+(c+d)(b+d)}{m^2}$</p>
<p>若，分类器 $h_i$ 与 $h_j$ 在D上完全一致，则 $\kappa=1$；若它们仅是偶然达成一致，则 $\kappa=0$。$\kappa$ 通常为非负值，仅在 $h_i$ 与 $h_j$ 达成一致的概率甚至低于偶然性的情况下取负值。</p>
</li>
</ul>
<h4 id="多样性增强">多样性增强</h4>
<p>在集成学习中需有效地生成多样性大的个体学习器。一般思路是在学习过程中引入随机性，常见做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。</p>
<ul>
<li>
<p>数据样本扰动</p>
<p>给定初始数据集，可从中产出不同的数据子集，再利用不同的数据子集训练出不同的个体学习器。数据样本扰动通常是基于采样法。</p>
<ul>
<li>效果明显：决策树、神经网络，对不稳定基学习器很有效。</li>
<li>效果不明显：线性学习、支持向量机、朴素贝叶斯、K邻近学习等，这样的基学习器称为 <strong>稳定基学习器（stable base learner）</strong>。</li>
</ul>
</li>
<li>
<p>输入属性扰动</p>
<p>训练样本通常由一组属性描述，不同的 <strong>子空间（subspace）</strong>，即属性子集提供了观察数据的不同视角。显然，从不同子空间训练出的个体学习器必然有所不同。</p>
</li>
<li>
<p>输出表示扰动</p>
<p>此类做法的基本思路是对输出表示进行操纵以增强多样性。可对训练样本的类标记稍作变动，如 <strong>翻转法（Flipping Output）</strong> 随机改变一些训练样本的标记；也可对输出表示进行转化，如 <strong>输出调制法（Output Smearing）</strong> 将分类输出转化为回归输出后构建个体学习器；还可将原任务拆解为多个可同时求解的子任务，如 ECOC法 利用纠错输出码将多分类任务拆解为一系列二分类任务来训练基学习器。</p>
</li>
<li>
<p>算法参数扰动</p>
<p>基学习算法一般都有参数需进行设置，例如神经网络的隐层神经元数、初始连接权值等，通过随机设置不同的参数，往往可以产生差别较大的个体学习器。</p>
</li>
</ul>
<p>不同的多样性增强机制可同时使用，有些方法甚至同时使用了更多机制。</p>
<h3 id="参考资料">参考资料</h3>
<ul>
<li><a href="https://baike.baidu.com/item/%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/3323240">泛化能力</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/38378225">集成模型概述(一)</a></li>
<li><strong>周志华《机器学习》西瓜书</strong></li>
<li><a href="https://www.cnblogs.com/liangjianli/p/11616847.html">LaTeX常用篇(二)&mdash;上下标/分式/根式/求和/连乘/极限/积分/希腊字母</a></li>
<li><a href="https://www.cnblogs.com/volcao/p/9483026.html">Hard Voting 与 Soft Voting 的对比</a></li>
<li><a href="https://blog.csdn.net/QFire/article/details/81382048">Latex各种命令、符号、公式、数学符号、排版等</a></li>
<li><a href="https://www.csdn.net/article/1970-01-01/2825965">集成模型的五个基础问题</a></li>
<li><a href="https://www.cnblogs.com/huangyc/p/9975183.html">集成学习（Ensemble Learning）Stacking</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1528864">Stacking 模型融合详解（附python代码）</a></li>
<li><a href="https://blog.csdn.net/data_scientist/article/details/78900265">搜狗用户画像-经验分享之stacking与blending（转）</a></li>
<li><a href="https://www.stat.berkeley.edu/~breiman/">Leo Breiman 1928&ndash;2005</a></li>
</ul>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">雨</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2020-11-24
        
    </span>
  </p>
  <p class="copyright-item">
      <span class="item-title">Markdown</span>
      <span class="item-content"><a class="link-to-markdown" href="https://yuallon.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95/index.md" target="_blank">The Markdown version »</a></span>
    </p>
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95/">集成方法</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">机器学习之基本概念</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/%E5%AE%B6/">
            <span class="next-text nav-default">家</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  <span id="/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95/" class="leancloud_visitors" data-flag-title="机器学习之集成方法">
		<span class="post-meta-item-text">文章阅读量 </span>
		<span class="leancloud-visitors-count">0</span>
		<p></p>
	  </span>
  <div id="vcomments"></div>
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
  <script type="text/javascript">
    new Valine({
        el: '#vcomments' ,
        appId: 'AYiTgQH0RnfHqVrLjAG62gYk-gzGzoHsz',
        appKey: 'OSOD331RCeVUC6vbdHNAO10t',
        notify:  false ,
        verify:  false ,
        avatar:'monsterid',
        placeholder: '愿我如星君如月，夜夜流光相皎洁！-- 宋·范成大',
        visitor:  true 
    });
  </script>

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:1318368321@qq.com" class="iconfont icon-email" title="email"></a>
      <a href="https://twitter.com/Yulong93522629" class="iconfont icon-twitter" title="twitter"></a>
      <a href="https://github.com/Yuallon" class="iconfont icon-github" title="github"></a>
      <a href="https://www.weibo.com/2115265873/profile?topnav=1&amp;wvr=6&amp;is_all=1" class="iconfont icon-weibo" title="weibo"></a>
      <a href="https://space.bilibili.com/26331996" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="https://yuallon.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2020<span class="heart"><i class="iconfont icon-heart"></i></span><span>雨</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
